{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "pressing-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "third-stewart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_model():\n",
    "    df = pd.read_csv('Review_Train2.csv')\n",
    "    heshe = stopwords.words('english')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    replace_wo_space = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    replace_with_space = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    reviews = df['review'].str.replace(replace_wo_space, '').str.lower()\n",
    "    reviews = reviews.str.replace(replace_with_space, '')\n",
    "    reviews_stopworded = reviews.apply(lambda x: ' '.join([word for word in str(x).split()\n",
    "                                                           if word not in heshe]))\n",
    "    lem = WordNetLemmatizer()\n",
    "    reviews_lem = reviews_stopworded.apply(lambda x: ' '.join([lem.lemmatize(word) for word in str(x).split()]))\n",
    "    ngram_cv = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "    ngram_cv.fit(reviews_lem)\n",
    "    X = ngram_cv.transform(reviews_lem)\n",
    "    target = df['Sentiment'].astype(str)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.2, random_state=42)\n",
    "    # I had to set the max_iter because it was not able to converge the SVM in the default 1,000 iterations\n",
    "    grid_params = {'C': [0.25, 0.5, 0.75, 1.0]}\n",
    "    svm = GridSearchCV(LinearSVC(multi_class='ovr', max_iter=1000000000, random_state=42), grid_params, cv=5)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_train_pred = svm.predict(X_train)\n",
    "    print(\"Training accuracy is {}\".format(accuracy_score(y_train, y_train_pred)))\n",
    "    y_test_pred = svm.predict(X_test)\n",
    "    print(\"Testing accuracy is {}\".format(accuracy_score(y_test, y_test_pred)))\n",
    "    # Save the vectorizer\n",
    "    vec_file = 'vectorizer.pickle'\n",
    "    pickle.dump(ngram_cv, open(vec_file, 'wb'))\n",
    "\n",
    "    # Save the model\n",
    "    mod_file = 'classification.model'\n",
    "    pickle.dump(svm, open(mod_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "removed-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_net_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "egyptian-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "    # tokenize text and remove puncutation\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    # remove words that contain numbers\n",
    "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "    # remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "    text = [x for x in text if x not in stop]\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    # pos tag text\n",
    "    pos_tags = pos_tag(text)\n",
    "    # lemmatize text\n",
    "    text = [WordNetLemmatizer().lemmatize(t[0], get_word_net_pos(t[1])) for t in pos_tags]\n",
    "    # remove words with only one letter\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    # join all\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "opponent-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_review(review):\n",
    "    # load the vectorizer\n",
    "    loaded_vectorizer = pickle.load(open('vectorizer.pickle', 'rb'))\n",
    "    # load the model\n",
    "    loaded_model = pickle.load(open('classification.model', 'rb'))\n",
    "    result = loaded_model.predict(loaded_vectorizer.transform([review]))\n",
    "    new_result = str(result)[1:-1]\n",
    "    sentiment = new_result.replace(\"'\", \"\")\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "acoustic-invitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Muhammad\n",
      "[nltk_data]     Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Muhammad\n",
      "[nltk_data]     Waseem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is 0.9933504670990903\n",
      "Testing accuracy is 0.9503997389459945\n"
     ]
    }
   ],
   "source": [
    "svm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "featured-machine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_review('I have edited three videos on this app so far and loved it until....I finished editing a video Feb 6th, uploaded it to YouTube, sent out the link to family and friends and it played fine. I went back to watch it this morning and my video clips within the video are missing. They played perfectly fine before. Very frustrating to put in so much work, everything be fine, and 2 days later my work is messed up.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-symphony",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-window",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
